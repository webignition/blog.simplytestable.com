---
layout: default
title: "Opensource Libraries We've Created and How We Use Them (Part Two Point One: HTTP Reliability)"
author:
    name: Jon Cram
    url: http://webignition.net
---

<div class="section">
    <p>
        I created some libraries for dealing with certain matters that would be common
        across the Simply Testable service.
    </p>
    <p>
        Such libraries address generic matters that should be useful to others. They
        have been opensource under an MIT license from day one.
    </p>     
    <p>
        In <a href="/opensource-libraries-weve-created-and-how-we-use-them-part-one-url-handling/">part one of this series</a>, I talked about what we use for handling URLs.
    </p>
    <p>
        In part two point one, I look at what we've created and use for handling HTTP
        requests <em>reliably</em>. Part two point two will expand on this and
        look at how we test HTTP-based applications.
    </p>
    <h2>
        Reliable HTTP requests
    </h2>
    <ul class="repository-list">
        <li>
            <i class="icon-github"></i>
            <a href="https://github.com/webignition/http-client">webignition/http-client</a>
            <a class="build-status" href="http://travis-ci.org/webignition/http-client"><img src="https://secure.travis-ci.org/webignition/http-client.png?branch=master" /></a>            
        </li>      
    </ul>
    <p>
        We need to query robots.txt and sitemap.xml or sitemap.txt files to get
        a list of URLs to be tested. We need to request every page of a site
        when testing to run a set of tests against every page.
    </p>
    <p>
        In short, we test the web. The web uses HTTP. We deal with a ton of HTTP requests.
    </p>    
    <p>
        Our <a href="https://github.com/webignition/http-client">HTTP client library</a>
        handles getting responses to HTTP requests in a way upon which we can rely.
    </p>
    <h2>The need for an HTTP client</h2>
    <p>
        In PHP, you can handle HTTP messaging in variety of ways. You can create
        your own HTTP layer on top of bare socket connections, you can use the 
        PECL HTTP extension or you can just take advantage of HTTP wrappers for
        file-based operations and pretend you're just reading a file.       
    </p>
    <p>
        We use the PECL HTTP extension which provides <a href="http://php.net/manual/en/class.httprequest.php">HTTPRequest</a>,
        <a href="http://php.net/manual/en/class.httpresponse.php">HTTPResponse</a>
        and <a hre="http://php.net/manual/en/class.httpmessage.php">HTTPMessage</a>
        objects for interacting with HTTP-based services.
    </p>
    <blockquote>
        <p>
           The HTTP extension eases handling of HTTP URLs, dates, redirects,
           headers and messages in a HTTP context (both incoming and outgoing).
        </p>
        <p>
           It also provides means for client negotiation of preferred language and
           charset, as well as a convenient way to exchange arbitrary data with
           caching and resuming capabilities. 
        </p>        
    </blockquote>    
    <p>
        Sounds like we can use the HTTP extension for all our HTTP needs. But try
        regularly handling many, many HTTP requests and you'll start to appreciate
        the brittleness of bare HTTP messaging.
    </p>   
    <p>
        An HTTP request is sent and an HTTP response containing what you expected
        is retrieved. This happens for the majority of requests.
    </p>
    <p>
        This doesn't happen for a minority of cases and that minority is
        significant. I'd say between 5-10% of HTTP requests get you back an HTTP
        response you didn't really expect. The response is often perfectly valid
        but may well not present the resource you hoped for.
    </p>
    <p>
        Some responses (quite validly) don't contain the resource you requested
        but instead contain a pointer to the resource you requested, with 301
        and 302 responses being most common.
    </p>      
    <p>
        You'll also notice that requests fail <em>all the time</em>.
    </p>    
    <p>       
        An HTTP conversation in isolation is subject to a range of failure modes.
        One of many DNS systems may be in a huff and the request to resolve a
        domain to an IP may timeout. An HTTP server may be busy and may take
        too long to respond to your request but may be perfectly able to respond
        quickly the next time you try. A complex HTTP application may fail and
        return a 500 error but may be perfectly able to respond as expected the
        next time you try.
    </p>
    <p>
        A isolated HTTP conversion is unreliable. For the odd occasional HTTP
        request such unreliabilty may go unnoticed. When regularly dealing with
        many, many HTTP requests and responses, unreliable is not good.
    </p>
    <h2>Making HTTP more reliable</h2>
    <p>
        You can send an <a href="http://php.net/manual/en/class.httprequest.php">HTTPRequest</a>
        and get directly back an <a hre="http://php.net/manual/en/class.httpmessage.php">HTTPMessage</a>
        containing what you want. Unless the resource you want is actually at another URL,
        either temporarily or permanently. Unless a DNS or HTTP server somewhere
        along the way isn't too busy right now. Unless an HTTP server somewhere
        along the way isn't faulty.
    </p>
    <p>
        Modern browsers spoil us, they make us feel that HTTP is reliable. You
        visit a URL and a page loads. And that page loads almost every single time
        unless there's a significant problem.
    </p>
    <p>
        You don't see how your browser follows redirect requests, how your browser
        doesn't give up the instant something times out, how your browser might
        be smart enough to try again for errors that, from experience, tend to be
        temporary.
    </p>
    <p>
        Our <a href="https://github.com/webignition/http-client">HTTP client library</a>
        handles all this for us. 
    </p>
    <p>
        You give it an HTTP request and it then gives back an HTTP message in
        response. Internally it follows redirects if you tell it to, it tries
        a few times if something times out and it tries again if it encounters
        an error which we've seen tends to be temporary.
    </p>
    <h2>The significance of making HTTP more reliable</h2>
    <p>
        Try crawling a site and try to keep going until you gather a collection of
        unique URLs for the entire site. You start at the homepage and gather
        all URLs relevant to the site that you find. For each of those URLs you do
        the same. Sounds straightforward, doesn't it?
    </p>
    <p>
        For very small sites with perhaps 10-20 unique URLs, you might get away
        with it without running into any timeouts or recoverable errors.
    </p>
    <p>
        For respectable-sized small sites with around 100 unique URLs, you might
        get away with it more often than not. The entire crawling process will fail
        maybe 1 in 10 times you try.
    </p>
    <p>
        For medium-sized sites with around 500-1000 unique URLs, you might get
        away with it if you're lucky. The entire crawling process will fail maybe
        9 times out of 10.
    </p>
    <p>
        For larger sites with 10,000+ unique URLs, you might get away with it once
        in a lifetime if you're particularly lucky. The entire crawling process will 
        fail every single time as far as you can tell.
    </p>
    <p>
        I can say this is the case having developed the tools our sitemap generator
        will be using. The sitemap generator crawls an entire site and gathers
        a collection of unique URLs. This is then used to figure out the URLs of a
        site that need testing.
    </p>
    <p>
        I've successfully managed to crawl <a hre="http://stackoverflow.com">Stack Overflow</a>
        to the point of collecting around 300,000 unique URLs before getting bored.
        Without any additional reliability mechanisms, this would be a once in a
        lifetime achievement.
    </p>
    <p>
        That's at the very least 300,000 HTTP requests in series that succeeded in
        retrieving the requested resource.
    </p>
    <p>
        That crawl was carried out over a 3G wireless  Internet service, purely to
        throw in a few more failure modes than you'd more commonly encounter for
        a wired server.
    </p>    
</div>